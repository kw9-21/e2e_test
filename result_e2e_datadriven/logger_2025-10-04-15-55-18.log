10/04 03:55:18 PM | Args: Namespace(lr=0.05, epochs=10, batch_size=256, save_dir='./result_e2e_datadriven', dataset_path='/home/dancer/wxr_code/e2e_prune/data', gpu_id='4', alpha_final=0.2, w_params_final=1.0, w_params_anneal_epochs=3, target_static_params_ratio=0.5, warmup_epochs=3, annealing_epochs_pct=0.7, dynamic_rate_start=0.7, dynamic_rate_end=0.7, gate_threshold=0.1, backbone='resnet50', target_flops_ratio=0.5)
10/04 03:55:20 PM | ==> Building full-size resnet50 for training and target calculation...
10/04 03:55:21 PM | ==> Accurately calculating FLOPs breakdown from a clean, original model...
10/04 03:55:23 PM | ==> FLOPs Sanity Check:
10/04 03:55:23 PM |   - Total FLOPs by Global Profile (Authoritative): 1559.586 M
10/04 03:55:23 PM |   - Total FLOPs by Sum of Parts:                 1373.558 M
10/04 03:55:23 PM |   - Discrepancy: 11.93%
10/04 03:55:23 PM |   - Sanity Check Warning: Discrepancy > 5%.
10/04 03:55:23 PM | ==> [DEBUG] FLOPs Breakdown Details:
10/04 03:55:23 PM |   - Input to layer1: (C,H,W)=(64,32,32)
10/04 03:55:23 PM |   - Input to layer2: (C,H,W)=(256,32,32)
10/04 03:55:23 PM |   - Input to layer3: (C,H,W)=(512,16,16)
10/04 03:55:23 PM |   - Input to layer4: (C,H,W)=(1024,8,8)
10/04 03:55:23 PM |   - conv1+bn1 FLOPs: 2.032 M
10/04 03:55:23 PM |   - layer1 FLOPs:     223.871 M
10/04 03:55:23 PM |   - layer2 FLOPs:     339.608 M
10/04 03:55:23 PM |   - layer3 FLOPs:     480.969 M
10/04 03:55:23 PM |   - layer4 FLOPs:     265.060 M
10/04 03:55:23 PM |   - fc FLOPs:         0.020 M
10/04 03:55:23 PM |   - Scalable total:   1311.560 M
10/04 03:55:23 PM |   - Overhead:gating   10.060 M
10/04 03:55:23 PM |   - Overhead:aux      51.937 M
10/04 03:55:23 PM |   - Overhead:budget   0.001 M
10/04 03:55:23 PM |   - Overhead total:   61.998 M
10/04 03:55:23 PM |   - Sum (Scalable+Overhead): 1373.558 M
10/04 03:55:23 PM | ==> Final FLOPs Targets:
10/04 03:55:23 PM |   - Max Theoretical FLOPs: 1559.59 M
10/04 03:55:23 PM |   - Target Total FLOPs (50.0%): 779.79 M
10/04 03:55:23 PM |   - Target Scalable FLOPs for Training: 715.74 M
10/04 03:55:23 PM | Full model total parameters: 44.341 M
10/04 03:55:23 PM | Target total parameters (50.0%): 22.171 M
10/04 03:55:23 PM | ==> Main model for training is ready.
10/04 03:55:23 PM | ==> Building main model for training...
10/04 03:55:24 PM | ==> Separating parameters into four groups for optimization...
10/04 03:55:24 PM |  - Main backbone params: 161 tensors.
10/04 03:55:24 PM |  - Gating (logit & static) params: 48 tensors.
10/04 03:55:24 PM |  - Auxiliary head params: 15 tensors.
10/04 03:55:24 PM |  - Budget generator params: 18 tensors.
10/04 03:55:24 PM | ==> Four optimizers created successfully.
10/04 03:55:24 PM | ==> LR scheduler set: 5-epoch linear warmup + Cosine Annealing.
10/04 03:55:24 PM | 
############################################################
10/04 03:55:24 PM | #####      STARTING UNIFIED ADAPTIVE TRAINING      #####
10/04 03:55:24 PM | Total Epochs: 10, Warmup Epochs: 3
10/04 03:55:24 PM | Loss Weights: w_aux=0.1, w_flops=2.0, w_params_final=1.0
10/04 03:55:24 PM | Target Total FLOPs: 779.79 MFLOPs
10/04 03:55:24 PM | Target Total FLOPs (Train): 715.74 MFLOPs
10/04 03:55:24 PM | ############################################################

10/04 03:55:24 PM | [Epoch 1/10] [WARMUP] LR: 0.004000 | w_params: 0.0000

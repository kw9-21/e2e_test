10/06 10:53:18 AM | Args: Namespace(lr=0.05, epochs=5, batch_size=128, save_dir='run_cifar10_deit', dataset_path='/home/dancer/wxr_code/e2e_prune/data', gpu_id='4', alpha_final=0.2, w_params_final=1.0, w_params_anneal_epochs=2, target_static_params_ratio=0.7, warmup_epochs=2, annealing_epochs_pct=0.7, dynamic_rate_start=0.7, dynamic_rate_end=0.7, gate_threshold=0.1, backbone='deit_small', bottleneck_strategy='A', dataset='cifar10', img_size=None, num_workers=4, imagenet_train_dir='/home/dancer/PublicData/ImageNet/train', imagenet_val_dir='/home/dancer/PublicData/ImageNet/val', target_flops_ratio=0.99)
10/06 10:53:20 AM | ==> Building full-size deit_small for training and target calculation...
10/06 10:53:21 AM | ==> FLOPs Sanity Check:
10/06 10:53:21 AM |   - Total FLOPs by Global Profile (Authoritative): 447.905 M
10/06 10:53:21 AM |   - Total FLOPs by Sum of Parts:                 447.905 M
10/06 10:53:21 AM |   - Discrepancy: 0.00%
10/06 10:53:21 AM | ==> [DEBUG] FLOPs Breakdown Details:
10/06 10:53:21 AM |   - [CoreOnly e2e]    426.602 M
10/06 10:53:21 AM |   - [Global-Overhead] 426.602 M
10/06 10:53:21 AM |   - [CoreOnly - (Fixed+ScalableOnly)] 0.000 M
10/06 10:53:21 AM |   - layer1 [MLP] FLOPs: 17.695 M
10/06 10:53:21 AM |   - layer2 [MLP] FLOPs: 17.695 M
10/06 10:53:21 AM |   - layer3 [MLP] FLOPs: 17.695 M
10/06 10:53:21 AM |   - layer4 [MLP] FLOPs: 17.695 M
10/06 10:53:21 AM |   - conv1+bn1 FLOPs: 320.430 M
10/06 10:53:21 AM |   - layer1 FLOPs:     8.847 M
10/06 10:53:21 AM |   - layer2 FLOPs:     8.847 M
10/06 10:53:21 AM |   - layer3 FLOPs:     8.847 M
10/06 10:53:21 AM |   - layer4 FLOPs:     8.847 M
10/06 10:53:21 AM |   - fc FLOPs:         0.004 M
10/06 10:53:21 AM |   - Scalable-only capacity total: 106.168 M
10/06 10:53:21 AM |   - Fixed total (conv1+bn1 + fc): 320.433 M
10/06 10:53:21 AM |   - Overhead:gating   5.322 M
10/06 10:53:21 AM |   - Overhead:static   0.000 M
10/06 10:53:21 AM |   - Overhead:aux      0.012 M
10/06 10:53:21 AM |   - Overhead:budget   0.003 M
10/06 10:53:21 AM |   - Overhead:misc     15.967 M
10/06 10:53:21 AM |   - Overhead total:   21.304 M
10/06 10:53:21 AM |   - Sum (Scalable+Fixed+Overhead): 447.905 M
10/06 10:53:21 AM | ==> Final FLOPs Targets:
10/06 10:53:21 AM |   - Max Theoretical FLOPs: 447.91 M
10/06 10:53:21 AM |   - Target Total FLOPs (99.0%): 443.43 M
10/06 10:53:21 AM |   - Target Scalable FLOPs for Training: 101.69 M
10/06 10:53:21 AM | Full model total parameters: 53.593 M
10/06 10:53:21 AM | Target total parameters (70.0%): 37.515 M
10/06 10:53:21 AM | ==> Main model for training is ready.
10/06 10:53:21 AM | ==> Building main model for training...
10/06 10:53:21 AM | ==> Separating parameters into four groups for optimization...
10/06 10:53:21 AM |  - Main backbone params: 200 tensors.
10/06 10:53:21 AM |  - Gating (logit & static) params: 120 tensors.
10/06 10:53:21 AM |  - Auxiliary head params: 6 tensors.
10/06 10:53:21 AM |  - Budget generator params: 18 tensors.
10/06 10:53:21 AM | ==> Four optimizers created successfully.
10/06 10:53:21 AM | ==> LR scheduler set: 5-epoch linear warmup + Cosine Annealing.
10/06 10:53:21 AM | 
############################################################
10/06 10:53:21 AM | #####      STARTING UNIFIED ADAPTIVE TRAINING      #####
10/06 10:53:21 AM | Total Epochs: 5, Warmup Epochs: 2
10/06 10:53:21 AM | Loss Weights: w_aux=0.1, w_flops=2.0, w_params_final=1.0
10/06 10:53:21 AM | Target Total FLOPs: 443.43 MFLOPs
10/06 10:53:21 AM | Target Total FLOPs (Train): 101.69 MFLOPs
10/06 10:53:21 AM | ############################################################

10/06 10:53:21 AM | [Epoch 1/5] [WARMUP] LR: 0.004000 | w_params: 0.0000
10/06 10:53:50 AM |  * Validation (Epoch 0): Acc@1 33.390 Acc@5 83.950
10/06 10:53:50 AM | ==> New best accuracy: 33.39% at epoch 1
10/06 10:53:52 AM | [Epoch 2/5] [WARMUP] LR: 0.013200 | w_params: 0.0000
10/06 10:54:17 AM |  * Validation (Epoch 1): Acc@1 36.190 Acc@5 86.680
10/06 10:54:17 AM | ==> New best accuracy: 36.19% at epoch 2
10/06 10:54:19 AM | [Epoch 3/5] [JOINT TRAINING] LR: 0.022400 | w_params: 0.0000
10/06 10:54:57 AM |  * Validation (Epoch 2): Acc@1 10.000 Acc@5 50.000
10/06 10:54:58 AM | [Epoch 4/5] [JOINT TRAINING] LR: 0.031600 | w_params: 0.5000
10/06 10:55:47 AM |  * Validation (Epoch 3): Acc@1 35.550 Acc@5 86.170
10/06 10:55:48 AM | [Epoch 5/5] [JOINT TRAINING] LR: 0.040800 | w_params: 1.0000
10/06 10:56:28 AM |  * Validation (Epoch 4): Acc@1 37.340 Acc@5 86.900
10/06 10:56:28 AM | ==> New best accuracy: 37.34% at epoch 5
10/06 10:56:31 AM | End-to-end training completed. Best validation accuracy: 37.340%
10/06 10:56:31 AM | 
############################################################
10/06 10:56:31 AM | #####      ANALYZING FINAL PRUNED MODEL STATS (UNIFIED)      #####
10/06 10:56:31 AM | ############################################################

10/06 10:56:31 AM | --- [Step 1] Baseline Full Model Stats ---
10/06 10:56:31 AM |   - Original Model Parameters: 53.593 M
10/06 10:56:31 AM |   - Original Model Max FLOPs:  447.905 M
10/06 10:56:31 AM | 
--- [Step 2] Final Static Structure Stats ---
10/06 10:56:31 AM |   - Statically Pruned Model Parameters: 26.998 M
10/06 10:56:31 AM | 
--- [Step 3] Calculating Average Dynamic FLOPs... ---
10/06 10:56:31 AM | --- [Unified FLOPs Calculator] Starting final analysis over validation set...
10/06 10:56:33 AM | --- [Unified FLOPs Calculator] Analysis complete.
10/06 10:56:33 AM |     - Average Total FLOPs: 442.597 M
10/06 10:56:33 AM | 
--- [Step 4] Pruning Analysis Summary ---
10/06 10:56:33 AM |   - Parameters: 53.593 M -> 26.998 M (Reduced by 49.62%)
10/06 10:56:33 AM |   - FLOPs:      0.448 G -> 0.443 G (Avg, Reduced by 1.19%)
10/06 10:56:33 AM | 
--- [Step 5] Final Performance Validation ---
10/06 10:56:35 AM |  * Final Performance Validation: Acc@1 37.340 Acc@5 86.900
10/06 10:56:35 AM | 
############################################################
10/06 10:56:35 AM | #####      ANALYSIS COMPLETE      #####
10/06 10:56:35 AM | ############################################################

10/06 10:56:35 AM | End-to-end pruning process finished successfully.
